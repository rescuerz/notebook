
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="rescuerz's notebook">
      
      
      
      
        <link rel="prev" href="../Lecture3_Linear_Classifiers/">
      
      
        <link rel="next" href="../Lecture5_Neural_Networks/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.31">
    
    
      
        <title>Optimization - rescuerz's notebook</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.3cba04c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/timeline.css">
    
      <link rel="stylesheet" href="../../supports/css/base.css">
    
      <link rel="stylesheet" href="../../supports/css/theme.css">
    
      <link rel="stylesheet" href="../../supports/css/admonitions.css">
    
      <link rel="stylesheet" href="https://jsd.cdn.zzko.cn/npm/katex@0.16.4/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#optimization" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="rescuerz&#39;s notebook" class="md-header__button md-logo" aria-label="rescuerz's notebook" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            rescuerz's notebook
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Optimization
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/rescuerz/Blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    rescuerz/Blog
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
    
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../ADS/Readme/" class="md-tabs__link">
          
  
    
  
  Computer Science

        </a>
      </li>
    
  

    
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../Lecture1_Introduction/" class="md-tabs__link">
          
  
    
  
  Artificial Intelligence

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="rescuerz&#39;s notebook" class="md-nav__button md-logo" aria-label="rescuerz's notebook" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    rescuerz's notebook
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/rescuerz/Blog" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    rescuerz/Blog
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../.." class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Home
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_1" id="__nav_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Home
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../changelog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    更新记录
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../link/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    友链
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Computer Science
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Computer Science
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    高级数据结构与算法分析
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            高级数据结构与算法分析
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ADS/Readme/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Readme
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ADS/1.%20AVL_tree/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AVL Tree &amp; Splay Tree
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Artificial Intelligence
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Artificial Intelligence
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Computer Vision
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture1_Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture2_Image_Classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image_Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture3_Linear_Classifiers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linear_Classifier
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Optimization
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Optimization
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#41-损失函数可视化" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 损失函数可视化
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#42-最优化-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 最优化 Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 最优化 Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421-策略-1随机搜索-random-search" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.1 策略 1：随机搜索 Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422-策略-2随机本地搜索" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.2 策略 2：随机本地搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#423-策略-3跟随梯度" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.3 策略 3：跟随梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#43-梯度计算" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 梯度计算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.3 梯度计算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#431-数值梯度法-numeric-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.1 数值梯度法 Numeric gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#432-微分分析计算梯度-analytic-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.2 微分分析计算梯度 Analytic gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#433-梯度下降" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.3 梯度下降
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#434-adagrad" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.4 AdaGrad
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#44-小结" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 小结
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture5_Neural_Networks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural Network
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture6_Backpropagation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Backpropagation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture7_ConvolutionalNetworks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Convolutional Neural Networks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture8_CNN_Architectures/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CNN Architecture
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture9_Hardware_and_Software/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hardware and Software
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#41-损失函数可视化" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 损失函数可视化
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#42-最优化-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 最优化 Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 最优化 Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421-策略-1随机搜索-random-search" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.1 策略 1：随机搜索 Random Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422-策略-2随机本地搜索" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.2 策略 2：随机本地搜索
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#423-策略-3跟随梯度" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.3 策略 3：跟随梯度
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#43-梯度计算" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 梯度计算
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.3 梯度计算">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#431-数值梯度法-numeric-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.1 数值梯度法 Numeric gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#432-微分分析计算梯度-analytic-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.2 微分分析计算梯度 Analytic gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#433-梯度下降" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.3 梯度下降
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#434-adagrad" class="md-nav__link">
    <span class="md-ellipsis">
      4.3.4 AdaGrad
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#44-小结" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 小结
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/rescuerz/Blog/blob/main/docs/CV/Lecture4_Optimization.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/rescuerz/Blog/raw/main/docs/CV/Lecture4_Optimization.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<h1 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h1>
<div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;">
<p><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8V2m6.78 1a.69.69 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38-2.5-2.5Z"/></svg></span> 约 6893 个字 <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3l89.3 89.4-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z"/></svg></span> 143 行代码 <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3L12.5 13Z"/></svg></span> 预计阅读时间 36 分钟</p>
</div>
<p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol>
<li>基于参数的 <strong>评分函数。</strong></li>
</ol>
<p>评分函数的主要作用是将输入图像的像素映射到分类评分值，这些评分值用于判断图像属于哪个类别。最常见的评分函数是线性模型，其形式如下：</p>
<p><span class="arithmatex">\(f(x;W,b)=Wx+b\)</span></p>
<ul>
<li><strong>x</strong> 是输入图像的像素值向量。</li>
<li><strong>W</strong> 是权重矩阵，每一行对应一个类别。</li>
<li><strong>b</strong> 是偏置向量。</li>
</ul>
<p>这个函数会输出每个类别的评分值（即得分）。这些得分并不直接代表概率，而是用于决定哪个类别的得分最高，因而认为该图像属于该类别。</p>
<ol>
<li><strong>损失函数</strong>。损失函数用于评估模型的预测与实际标签之间的差异，并引导模型参数的更新。损失函数通过衡量分类评分和实际分类的一致性，来评估参数的好坏。两种常见的损失函数是 <strong>SVM Loss</strong> 和 <strong>Softmax Loss</strong>。</li>
</ol>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171548533.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240817154823376" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171548533.png" /></a></p>
<p>SVM 损失函数的目标是最大化正确分类的得分与其他类别的得分之间的差距。SVM 损失函数通常表达为：</p>
<div class="arithmatex">\[
L = \dfrac{1}{N}\sum_{i}\sum_{j!= y_i}[max(0, f(x_i, W)_j - f(x_i, W)_{y_i} + \Delta)] + \alpha R(W)\\
 L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(y_i\)</span>是正确类别的索引。</li>
<li><span class="arithmatex">\(s_j\)</span>是第 <span class="arithmatex">\( j \)</span> 类的得分。</li>
<li><span class="arithmatex">\(\Delta\)</span>是一个超参数，控制得分差距的容忍度。</li>
</ul>
<p><strong>SVM 损失函数希望正确类别的得分比其他类别的得分高至少 <span class="arithmatex">\(\Delta\)</span>，否则会产生损失。</strong></p>
<p>Softmax 损失函数将得分转化为概率分布，并使用交叉熵来衡量预测概率与实际标签分布的差异。其表达式为：</p>
<div class="arithmatex">\[
L_i = -\log \left(\dfrac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)
\]</div>
<ul>
<li><span class="arithmatex">\(s_{y_i}\)</span>是正确类别的得分。</li>
<li><span class="arithmatex">\(\sum_j e^{s_j}\)</span>是所有类别得分的指数和。</li>
</ul>
<p><strong>Softmax 损失函数希望正确类别的概率接近 1，同时其他类别的概率接近 0。</strong></p>
<p>对于图像数据 <span class="arithmatex">\(x_i\)</span>，如果基于参数集 <span class="arithmatex">\(W\)</span>做出的分类预测与真实情况比较一致，那么计算出来的损失值 <span class="arithmatex">\(L\)</span>就很低。现在介绍第三个，也是最后一个关键部分：<strong><code>最优化Optimization。最优化是寻找能使得损失函数值最小化的参数$W$的过程。</code></strong></p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="41-损失函数可视化">4.1 损失函数可视化<a class="headerlink" href="#41-损失函数可视化" title="Permanent link">&para;</a></h2>
<p>在高维空间中，像图像分类任务中的损失函数是难以直接可视化的，因为它们涉及大量的参数。例如，对于 CIFAR-10 图像分类中的线性分类器，其权重矩阵有 30,730 个参数（对于每个类 3,073 个参数，共 10 个类）。然而，通过在低维空间中对高维损失函数进行切片，可以获取其结构的直观理解。这种方法能帮助我们了解损失函数的复杂性及其对不同参数的敏感度。</p>
<ol>
<li>单维度的切片可视化</li>
</ol>
<p>可以 <strong>选取一个随机方向</strong> 并沿该方向进行探索，生成一维图像。具体来说：</p>
<ul>
<li>首先，选择一个随机生成的权重矩阵 W 作为起点。</li>
<li>然后，沿某个随机方向 <span class="arithmatex">\(W_1\)</span>前进，记录损失值 <span class="arithmatex">\(L(W + aW_1)\)</span>随 a 变化的情况。</li>
</ul>
<p><strong>左图</strong> 展示了这个过程：x 轴表示步长 a，y 轴表示对应的损失值。由于 SVM 损失函数的分段线性性质，我们会看到多个线性段连接形成的图像，反映了模型对不同权重配置的敏感度。</p>
<ol>
<li>双维度的切片可视化</li>
</ol>
<p>为了获得更丰富的结构信息，可以在两个维度上对损失函数进行切片：</p>
<ul>
<li>生成两个随机方向 <span class="arithmatex">\(W_1\)</span>和 <span class="arithmatex">\(W_2\)</span>。</li>
<li>计算不同 <span class="arithmatex">\(a, b\)</span>组合下的损失值 <span class="arithmatex">\(L(W + aW_1 + bW_2)\)</span></li>
</ul>
<p><strong>中图和右图</strong> 展示了在二维平面上的切片结果。x 轴和 y 轴分别代表 a 和 b 的值，颜色代表损失值的大小。蓝色区域表示损失较低，红色区域表示损失较高。</p>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171556769.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="img" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171556769.png" /></a></p>
<p>一个无正则化的多类 SVM 的损失函数的图示。左边和中间只有一个样本数据，右边是 CIFAR-10 中的 100 个数据。<strong>左</strong>：a 值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。</p>
<p><strong>当只使用一个样本进行计算（如左图和中图），损失函数具有明显的 <code>分段线性特征</code></strong>，反映了 SVM 的性质。而当使用 <code>更多样本</code> 时（如右图），损失函数的图像变得更加平滑，因为每个样本的分段线性特征被平均化，形成了一个类似“碗状”的结构。<strong><code>这种平滑的“碗”结构有助于理解损失函数的最小值处于何处，即模型找到一个对所有样本都表现良好的权重配置</code></strong>。</p>
<hr />
<blockquote>
<p>线性结构是由每个样本的损失值通过 <code>max(0, -)</code> 函数计算得出的，它将线性函数的结果与 0 进行比较，从而产生多个分段线性部分。</p>
<p>具体来说，对于单个数据点的损失函数 <span class="arithmatex">\(L_i\)</span>而言，它是所有其他类别的得分减去正确类别得分再加上一个常数（通常为 1）的最大值的和。因此，<strong><code>损失函数是 W 的线性函数的总和</code></strong>，每个线性部分通过 <code>max</code> 操作被截断在零值以上。</p>
</blockquote>
<p>对于一个单独的数据，有损失函数的计算公式如下：</p>
<div class="arithmatex">\[
L_i = \sum_{j!= y_i}[max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1)]
\]</div>
<p>通过公式可见，每个样本的数据损失值是以 <span class="arithmatex">\(W\)</span>为参数的线性函数的总和（零阈值来源于 <span class="arithmatex">\(max(0,-)\)</span>函数）。<span class="arithmatex">\(W\)</span>的每一行（即 <span class="arithmatex">\(W_j\)</span>），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有 3 个只有 1 个维度的点，数据集数据点有 3 个类别。那么完整的无正则化 SVM 的损失值计算如下：</p>
<div class="arithmatex">\[
L_0 = max(0, w^T_1x_0-w^T_0x_0+1)+max(0, w^T_2x_0-w^T_0x_0+1)\\
L_1 = max(0, w^T_0x_1-w^T_1x_1+1)+max(0, w^T_2x_1-w^T_1x_1+1)\\
L_2 = max(0, w^T_0x_2-w^T_2x_2+1)+max(0, w^T_1x_2-w^T_2x_2+1)\\
L =(L_0+L_1+L_2)/3
\]</div>
<p>因为这些例子都是一维的，所以数据 <span class="arithmatex">\(x_i\)</span>和权重 <span class="arithmatex">\(w_j\)</span>都是数字。观察 <span class="arithmatex">\(w_0\)</span>，可以看到上面的式子中一些项是 <span class="arithmatex">\(W_0\)</span>的线性函数，且每一项都会与 0 比较，取两者的最大值。可作图如下：——————————————————————————————————————</p>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171600171.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="img" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171600171.png" /></a></p>
<p>从一个维度方向上对数据损失值的展示。x 轴方向就是一个权重，y 轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与 0 阈值的比较。完整的 SVM 数据损失就是这个形状的 30730 维版本。</p>
<hr />
<p>在多维情况下，类似的分段线性结构会出现在每个维度上，从而构成高维空间中的凸形状。正因为如此，SVM 损失函数是凸函数，这使得其优化问题在数学上是可解的，并且可以应用有效的凸优化算法。但是一旦我们将函数 <span class="arithmatex">\(f\)</span>扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</p>
<h2 id="42-最优化-optimization">4.2 最优化 Optimization<a class="headerlink" href="#42-最优化-optimization" title="Permanent link">&para;</a></h2>
<p>在凸优化中，像 SVM 损失函数这样的凸函数可以通过梯度下降或其他凸优化算法有效地找到全局最优解。然而，当我们转向神经网络时，损失函数通常是非凸的，这意味着它们可能包含多个局部极小值、鞍点以及复杂的地形。在这种情况下，标准的凸优化方法可能不再适用，我们需要更复杂的优化策略，<strong>如随机梯度下降（SGD）、动量方法、自适应学习率优化器（如 Adam）等。</strong></p>
<h3 id="421-策略-1随机搜索-random-search">4.2.1 策略 1：随机搜索 Random Search<a class="headerlink" href="#421-策略-1随机搜索-random-search" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>随机搜索的策略</strong>：通过随机生成大量不同的权重矩阵，计算每个权重矩阵的损失值，并选取损失值最小的那个权重矩阵作为最终的模型。</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="c1"># 假设 X_train 的每一列代表一个数据样本（形状为 3073 x 50000）</span>
<span class="c1"># 假设 Y_train 是对应的标签数组（长度为 50000）</span>
<span class="c1"># 假设函数 L 计算给定权重 W 的损失值</span>

<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>  <span class="c1"># 初始化最小损失值为正无穷</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>  <span class="c1"># 进行 1000 次随机搜索</span>
    <span class="c1"># 生成一个形状为 10x3073 的随机权重矩阵，并缩放。权重矩阵中的值由标准正态分布生成，并乘以一个很小的常数（0.0001）以确保初始权重较小</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># 计算当前权重 W 对整个训练集的损失值</span>
    <span class="c1"># 更新最佳损失和最佳权重</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>  <span class="c1"># 如果当前损失值小于记录的最小损失值</span>
        <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>  <span class="c1"># 更新最小损失值</span>
        <span class="n">bestW</span> <span class="o">=</span> <span class="n">W</span>  <span class="c1"># 更新最佳权重矩阵</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;在第 </span><span class="si">%d</span><span class="s1"> 次尝试中的损失值为 </span><span class="si">%f</span><span class="s1">，当前最小损失值为 </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">))</span>

<span class="c1"># 输出示例：</span>
<span class="c1"># 在第 0 次尝试中的损失值为 9.401632，当前最小损失值为 9.401632</span>
<span class="c1"># 在第 1 次尝试中的损失值为 8.959668，当前最小损失值为 8.959668</span>
<span class="c1"># ... (此处省略，代码会持续输出 1000 行)</span>

<span class="c1"># 使用训练集上表现最好的权重矩阵 W 在测试集上进行预测</span>
<span class="c1"># 假设 X_test 形状为 3073 x 10000，Y_test 为长度为 10000 的标签数组</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">bestW</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xte_cols</span><span class="p">)</span>  <span class="c1"># 计算所有测试样本的分类得分（形状为 10 x 10000）</span>
<span class="n">Yte_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 选择每个样本得分最高的类别作为预测结果</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yte_predict</span> <span class="o">==</span> <span class="n">Yte</span><span class="p">)</span>  <span class="c1"># 计算预测结果与真实标签的匹配比例（准确率）</span>
<span class="c1"># 返回的准确率为 0.1555</span>
</code></pre></div>
<p>核心思路：迭代优化</p>
<ul>
<li><strong>局限性</strong>：找到全局最优解非常困难，尤其是在参数空间维度非常高的情况下（如神经网络的权重）。</li>
<li><strong>改进思路</strong>：<strong><code>将优化问题分解为一系列更小的、迭代的步骤，每一步都试图略微减少损失值，而不是试图直接找到最优解。通过逐步改进权重矩阵，损失值会逐渐减少，从而达到更好的性能。</code></strong></li>
</ul>
<h3 id="422-策略-2随机本地搜索">4.2.2 策略 2：随机本地搜索<a class="headerlink" href="#422-策略-2随机本地搜索" title="Permanent link">&para;</a></h3>
<p>第一个策略可以看做是每走一步都尝试几个随机方向，<strong>如果某个方向是向山下的，就向该方向走一步。</strong>这个策略基于局部探索，通过微调初始随机权重矩阵来减少损失值。具体来说，从一个随机权重矩阵 <span class="arithmatex">\(W\)</span>开始，然后添加一个小扰动 <span class="arithmatex">\(\delta W\)</span>，生成新的权重矩阵 <span class="arithmatex">\(W_{\text{try}} = W + \delta W\)</span>。如果新权重的损 失值比当前最优值更小，就更新权重 <span class="arithmatex">\(W\)</span>为 <span class="arithmatex">\(W_{\text{try}}\)</span>。每次尝试随机移动一点，只在找到更低损失时更新位置。</p>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171602466.jpeg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171602466.jpeg" /></a></p>
<div class="highlight"><pre><span></span><code><span class="n">W</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># 生成一个随机的初始权重矩阵 W</span>
<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>  <span class="c1"># 初始化最小损失值为正无穷</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>  <span class="c1"># 进行 1000 次局部搜索</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.0001</span>  <span class="c1"># 设置步长，控制扰动的大小</span>
    <span class="c1"># 每次迭代时，生成一个与当前权重同维度的随机扰动 Wtry，并通过步长 step_size 来控制扰动的幅度。这种小幅度的扰动确保了模型在每一步中仅对参数进行微调。</span>
    <span class="n">Wtry</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>  <span class="c1"># 在当前权重基础上添加随机扰动</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">Xtr_cols</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Wtry</span><span class="p">)</span>  <span class="c1"># 计算扰动后权重的损失值</span>

    <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>  <span class="c1"># 如果新的损失值比之前更小</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">Wtry</span>  <span class="c1"># 更新权重矩阵为新的权重</span>
        <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>  <span class="c1"># 更新最小损失值</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;第 </span><span class="si">%d</span><span class="s1"> 次迭代的损失值为 </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">))</span>
</code></pre></div>
<p>使用同样的数据（1000），这个方法可以得到 <strong>21.4%</strong> 的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</p>
<h3 id="423-策略-3跟随梯度">4.2.3 策略 3：跟随梯度<a class="headerlink" href="#423-策略-3跟随梯度" title="Permanent link">&para;</a></h3>
<p>在之前的策略中，我们随机寻找一个能降低损失的方向，而这里我们直接计算出最陡峭的下降方向——损失函数的 <strong>梯度</strong>。梯度告诉我们在多维空间中，各个方向上的变化率，通过沿着梯度方向更新参数，我们能够更快速地接近最优解。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数 <strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<div class="arithmatex">\[
\dfrac{df(x)}{dx}= lim_{h \rightarrow 0}\dfrac{f(x+h)-f(x)}{h}
\]</div>
<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="43-梯度计算">4.3 梯度计算<a class="headerlink" href="#43-梯度计算" title="Permanent link">&para;</a></h2>
<p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<h3 id="431-数值梯度法-numeric-gradient">4.3.1 数值梯度法 Numeric gradient<a class="headerlink" href="#431-数值梯度法-numeric-gradient" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>数值梯度法是一种通过有限差分近似计算梯度的方法。虽然相比分析梯度法它较慢，但实现简单且通用，适合用于调试和验证分析梯度的正确性。其核心思想是通过在每个维度上施加一个微小的扰动，然后观察函数值的变化，来近似计算该维度上的导数。</strong></p>
</blockquote>
<p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数 <strong>f</strong> 和向量 <strong>x，</strong> 计算 <strong>f</strong> 的梯度的通用函数，它返回函数 <strong>f</strong> 在点 <strong>x 处</strong> 的梯度.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算函数 f 在点 x 处的数值梯度。</span>

<span class="sd">    参数：</span>
<span class="sd">    - f: 目标函数，输入为 x，输出为一个标量</span>
<span class="sd">    - x: numpy 数组，表示计算梯度的点</span>

<span class="sd">    返回：</span>
<span class="sd">    - grad: 与 x 形状相同的 numpy 数组，表示 f 在 x 处的梯度</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 计算函数在原点的值</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 初始化与 x 形状相同的梯度数组,用于存储梯度</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-5</span>  <span class="c1"># 设置一个非常小的数值，作为扰动</span>

    <span class="c1"># 使用 nditer 迭代 x 中的所有索引</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>  <span class="c1"># 获取当前索引</span>
        <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># 保存当前值</span>

        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span>  <span class="c1"># 在当前维度增加 h</span>
        <span class="n">fxh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 计算 f(x + h)</span>

        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span>  <span class="c1"># 恢复原值</span>

        <span class="c1"># 计算该维度上的偏导数</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>  <span class="c1"># 梯度计算公式</span>
        <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>  <span class="c1"># 移动到下一个维度</span>

    <span class="k">return</span> <span class="n">grad</span>
</code></pre></div>
<blockquote>
<p><strong>迭代计算：</strong> 对于 <code>x</code> 中的每个维度，施加一个微小的正向扰动 <code>h</code>，计算扰动后函数的值 <code>f(x + h)</code>。通过比较原函数值 <code>f(x)</code> 和扰动后函数值的变化，计算该维度上的梯度。</p>
</blockquote>
<p><strong>实践考量</strong>：注意在数学公式中，<strong>h</strong> 的取值是趋近于 0 的，然而在实际中，用一个很小的数值（比如例子中的 1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的 h。</p>
<ul>
<li><strong>中心差分法：</strong> 使用 <code>f(x + h)</code> 和 <code>f(x - h)</code> 的中心差分公式来计算梯度，效果更好，可以减少误差。公式为：<span class="arithmatex">\(\dfrac{f(x + h) - f(x - h)}{2h}\)</span></li>
</ul>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10 损失函数的梯度：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 定义一个封装的 CIFAR-10 损失函数，便于传递给数值梯度函数</span>
<span class="k">def</span> <span class="nf">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="c1"># 生成一个随机的权重矩阵 W</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>

<span class="c1"># 计算损失函数在 W 处的梯度</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">CIFAR10_loss_fun</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 打印初始损失值</span>
<span class="n">loss_original</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;original loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_original</span><span class="p">,))</span>

<span class="c1"># 尝试不同的步长（学习率）并观察对损失值的影响</span>
<span class="k">for</span> <span class="n">step_size_log</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">step_size_log</span>  <span class="c1"># 根据对数值生成步长</span>
    <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">df</span>  <span class="c1"># 向负梯度方向更新权重</span>
    <span class="n">loss_new</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>  <span class="c1"># 计算新的损失值</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;for step size </span><span class="si">%f</span><span class="s1"> new loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">loss_new</span><span class="p">))</span>

<span class="c1"># 输出结果显示了不同步长对应的损失值变化</span>
<span class="c1"># original loss: 2.200718</span>
<span class="c1"># for step size 1.000000e-10 new loss: 2.200652</span>
<span class="c1"># for step size 1.000000e-09 new loss: 2.200057</span>
<span class="c1"># for step size 1.000000e-08 new loss: 2.194116</span>
<span class="c1"># for step size 1.000000e-07 new loss: 2.135493</span>
<span class="c1"># for step size 1.000000e-06 new loss: 1.647802</span>
<span class="c1"># for step size 1.000000e-05 new loss: 2.844355</span>
<span class="c1"># for step size 1.000000e-04 new loss: 25.558142</span>
<span class="c1"># for step size 1.000000e-03 new loss: 254.086573</span>
<span class="c1"># for step size 1.000000e-02 new loss: 2539.370888</span>
<span class="c1"># for step size 1.000000e-01 new loss: 25392.214036</span>
</code></pre></div>
<blockquote>
<ul>
<li>
<p><strong>梯度方向：</strong> 更新权重时，向着梯度的负方向移动，即 <code>W_new = W - step_size * df</code>，因为梯度指向的是损失函数上升最快的方向，向其反方向移动可以降低损失值。</p>
</li>
<li>
<p><strong>步长选择：</strong> 步长决定了每次更新权重时移动的距离。步长过小，下降稳定但进度慢；步长过大，可能越过最优点导致更高的损失值。</p>
</li>
</ul>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171605810.jpeg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="img" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171605810.jpeg" /></a></p>
<p>将步长效果视觉化的图例。从某个具体的点 W 开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为 <strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p>
<ul>
<li><strong>效率问题</strong>：计算数值梯度的复杂性和参数的量线性相关。在本例中有 30730 个参数，所以损失函数每走一步就需要计算 30731 次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</li>
</ul>
</blockquote>
<h3 id="432-微分分析计算梯度-analytic-gradient">4.3.2 微分分析计算梯度 Analytic gradient<a class="headerlink" href="#432-微分分析计算梯度-analytic-gradient" title="Permanent link">&para;</a></h3>
<p>相比有限差值近似法，利用微分公式计算梯度更加精确且计算效率更高。我们可以直接从损失函数的公式推导出梯度的解析表达式，再用它进行优化。这种方法的优点是计算精度高、速度快；但缺点是推导复杂、容易出错。因此，实际操作中常用 <strong>梯度检查</strong> 来验证解析梯度的正确性。</p>
<p>用 SVM 的损失函数在某个数据点上的计算来举例：</p>
<div class="arithmatex">\[
\sum_{j!= y_i}max(0, w^T_jx_i-w^T_{y_i}x_i+\Delta)
\]</div>
<p>可以对函数进行微分。比如，对 <span class="arithmatex">\(w_{y_i}\)</span>进行微分得到：</p>
<div class="arithmatex">\[
\nabla {w_{y_i}}L_i =-(\sum_{j\not = y_i}1(w^T_jx_i-w^T_{y_i}x_i+Delta &gt; 0))x_i
\]</div>
<p>该梯度的计算涉及一个指示函数，当括号内的条件为真时，其值为 1，否则为 0。只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以 <span class="arithmatex">\(x_i\)</span>就是梯度了。注意，这个梯度只是对应正确分类的 W 的行向量的梯度，那些 <span class="arithmatex">\(j\not =y_i\)</span>行的梯度是：</p>
<div class="arithmatex">\[
\nabla_{w_j}L_i = 1(w^T_jx_i-w^T_{y_i}x_i+Delta &gt; 0)x_i
\]</div>
<h3 id="433-梯度下降">4.3.3 梯度下降<a class="headerlink" href="#433-梯度下降" title="Permanent link">&para;</a></h3>
<p>梯度下降法的核心思想，即通过不断计算梯度并更新参数，使得损失函数逐渐降低。</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 普通的梯度下降</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>  <span class="c1"># 计算当前权重的梯度</span>
    <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span>  <span class="c1"># 更新权重，沿梯度的负方向移动</span>
</code></pre></div>
<p>梯度下降是一种迭代优化算法，其目标是找到一个使损失函数最小化的参数集合。通过沿着损失函数梯度的负方向更新参数，算法试图在每一步中尽可能地降低损失。</p>
<ul>
<li>
<p><strong>梯度计算</strong>：在每次迭代中，通过 <code>evaluate_gradient</code> 函数计算损失函数相对于参数的梯度 <code>weights_grad</code>。梯度指向损失函数上升最快的方向。</p>
</li>
<li>
<p><strong>参数更新</strong>：权重 <code>weights</code> 按照学习率(learning rate) <code>step_size</code> 沿着负梯度方向更新。这样做是因为负梯度方向是损失减少最快的方向。</p>
</li>
<li>
<blockquote>
<p>这个循环结构会持续运行，直到满足某种停止条件。实际应用中，常见的停止条件包括：</p>
<ul>
<li><strong>固定迭代次数</strong>：运行指定次数的迭代，例如 1000 次。</li>
<li><strong>损失变化小于某个阈值</strong>：当损失的变化小于设定的阈值时，认为已收敛。</li>
<li><strong>梯度范数很小</strong>：当梯度的范数（即梯度向量的长度）非常小时，说明已经接近极小值点。</li>
</ul>
</blockquote>
</li>
</ul>
<p>Batch Gradient Descent 批量梯度下降</p>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191046238.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819104619162" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191046238.png" /></a></p>
<p><strong>小批量数据梯度下降（Mini-batch Gradient Descent, MBGD）</strong> 是介于批量梯度下降（Batch Gradient Descent, BGD）和随机梯度下降（Stochastic Gradient Descent, SGD）之间的一种优化策略。它在每次迭代时使用一部分（即<strong>小批量</strong>）数据来计算梯度并更新参数。这种方法结合了 BGD 和 SGD 的优点，在大规模机器学习应用中广泛使用。</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 普通的小批量数据梯度下降</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">data_batch</span> <span class="o">=</span> <span class="n">sample_training_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># 256个数据</span>
    <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>  <span class="c1"># 计算梯度</span>
    <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span>  <span class="c1"># 参数更新</span>
</code></pre></div>
<ul>
<li>
<p><strong><code>sample_training_data(data, 256)</code></strong>：从数据集中随机选取 256 个样本，作为当前迭代的小批量数据。</p>
</li>
<li>
<p><strong><code>evaluate_gradient(loss_fun, data_batch, weights)</code></strong>：计算小批量数据上的损失函数梯度。</p>
</li>
</ul>
<blockquote>
<p><strong><code>小批量数据梯度下降的工作原理</code></strong></p>
<ul>
<li><strong>数据批次</strong>：小批量数据的典型大小通常为 32、64、128 或 256 个样本。每次迭代时，从训练集中随机选取一个小批量数据来计算梯度。</li>
</ul>
<p><code>参数数量的选择和选择参数是hyper parameter，但是影响不大</code></p>
<ul>
<li><strong>梯度计算与更新</strong>：在每次迭代中，使用小批量数据计算梯度，随后更新模型参数。由于每次迭代使用的只是数据的一小部分，计算速度比全批量更新快得多，同时又比单个样本的更新更稳定。</li>
</ul>
<p><strong><code>小批量数据梯度下降的优势</code></strong></p>
<ul>
<li>
<p><strong>计算效率高</strong>：相比 BGD，MBGD 的计算开销更小，因为它不需要在每次迭代时计算整个数据集的梯度，而是从训练集中随机取出一部分来计算梯度，近似整个数据集的梯度。而与 SGD 相比，MBGD 的梯度估计更准确，减少了参数更新中的噪声。</p>
</li>
<li>
<p><strong>平衡收敛与计算开销</strong>：MBGD 结合了 BGD 的稳定性和 SGD 的快速收敛性，在计算成本和模型性能之间取得平衡。</p>
</li>
</ul>
</blockquote>
<p>小批量数据策略有个极端情况，那就是每个批量中只有 1 个数据样本，这种策略被称为 <strong>随机梯度下降（Stochastic Gradient Descent 简称 SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算 100 个数据 比 100 次计算 1 个数据要高效很多。</p>
<p>小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如 32，64，128 等。<strong>之所以使用 2 的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是 2 的倍数，那么运算更快。并且在批量大小合适的情况下，可以充分利用计算</strong></p>
<p><strong>两个潜在问题：</strong></p>
<ul>
<li>学习率如果过大，那么就会出现震荡的情况，学习率过小的话，虽然没有震荡情况出现，但是学习速度会很慢，这个问题在技术上有时候被称为具有高条件数的问题</li>
</ul>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191053481.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819105344402" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191053481.png" /></a></p>
<ul>
<li>存在局部最小点和鞍点 saddle point（<strong><code>鞍点指的是某些维度上的梯度为零，但其他维度仍需优化，使得模型无法继续向下优化</code></strong>），在某些维度上可能无法优化，也就是高维度下优化难题，这样模型无法达到最佳。</li>
</ul>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191055693.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819105521626" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191055693.png" /></a></p>
<p>或者可以使用动量法进行优化，尝试越过鞍点和局部最小点，会产生很不错的效果，同时还可以抑制震荡，是一种很有用的方法</p>
<p>动量法可以视为给梯度下降添加了“惯性”，使得更新方向不仅取决于当前的梯度，还受过去更新方向的影响。公式如下：</p>
<div class="arithmatex">\[
v = \gamma v - \eta \nabla_\theta J(\theta)\\
\theta = \theta + v
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\(\gamma\)</span>是动量因子，通常设为 0.9。</li>
<li><span class="arithmatex">\(v\)</span>是累积的动量。</li>
<li><span class="arithmatex">\(eta\)</span>是学习率。</li>
<li><span class="arithmatex">\(\nabla_\theta J(\theta)\)</span>是当前梯度。</li>
<li><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191059838.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819105913771" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191059838.png" /></a></li>
<li><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191101117.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819110142018" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191101117.png" /></a></li>
</ul>
<p>通过累积动量，模型能够跨越鞍点，并在陡峭的区域加速收敛，在平坦的区域减缓震荡。</p>
<h3 id="434-adagrad">4.3.4 AdaGrad<a class="headerlink" href="#434-adagrad" title="Permanent link">&para;</a></h3>
<p>AdaGrad（Adaptive Gradient Algorithm）是一种自适应学习率的方法，它通过调整每个参数的学习率来提升优化的效率。与标准的梯度下降不同，AdaGrad 会根据历史梯度的累积值动态调整学习率，使得更新的幅度能够适应不同的参数。</p>
<blockquote>
<p>原理：在每次迭代中，AdaGrad 会累加梯度的平方值，并将这个累积值用于调整学习率。由于梯度的平方值逐步增加，学习率会逐步减少，从而在训练过程中实现自适应调整。</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="n">grad_squared</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>         <span class="c1"># 计算梯度</span>
    <span class="n">grad_squared</span> <span class="o">+=</span> <span class="n">dw</span> <span class="o">*</span> <span class="n">dw</span>          <span class="c1"># 累加梯度的平方和</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_squared</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>  <span class="c1"># 自适应学习率更新</span>
</code></pre></div>
<blockquote>
<p>优点：</p>
<ul>
<li><strong>适应稀疏数据</strong>：AdaGrad 特别适合处理稀疏数据，因为它可以根据历史梯度动态调整学习率，使得频繁更新的参数学习率逐渐减小，而不常更新的参数则保留较大的学习率。</li>
<li><strong>无需手动调整学习率</strong>：通过自动调节学习率，AdaGrad 减轻了手动调参的负担。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>过度衰减</strong>：由于学习率随时间单调减少，AdaGrad 在训练的后期可能会导致学习率过小，进而使得模型难以进一步优化。</li>
</ul>
</blockquote>
<p><strong>RMSprop 的改进：</strong></p>
<p>为了解决 AdaGrad 的学习率不断下降问题，RMSprop 在计算梯度平方累积时引入了指数加权移动平均（decay rate），避免累积过多的历史信息，从而让学习率在训练过程中保持相对稳定。RMSprop 在处理非凸问题时更为有效。</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_squared</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">grad_squared</span> <span class="o">=</span> <span class="n">decay_date</span> <span class="o">*</span> <span class="n">grad_squared</span> <span class="o">+</span> <span class="err">（</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">*</span> <span class="n">dw</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_squared</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>  <span class="c1"># 自适应学习率更新</span>
</code></pre></div>
<p><strong>decay_rate</strong>: 这个参数控制历史梯度的衰减速度，使优化算法对新梯度信息更加敏感。常见值是 0.9。</p>
<p><strong>Adam:RMSProp + Momentum</strong></p>
<p>Adam 结合了 RMSprop 和动量（Momentum）的优点，不仅考虑梯度的均值，还考虑梯度的方差。Adam 通过一阶矩估计（动量）和二阶矩估计（梯度平方的指数移动平均）自适应地调整学习率，在处理非凸优化问题和噪声数据时非常稳健，并且通常不需要过多的超参数调整。</p>
<div class="highlight"><pre><span></span><code><span class="n">moment1</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 初始化一阶动量</span>
<span class="n">moment2</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 初始化二阶动量</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># 计算当前参数的梯度</span>
    <span class="n">moment1</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">moment1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span>  <span class="c1"># 更新一阶动量（类似于动量）</span>
    <span class="n">moment2</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">moment2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">*</span> <span class="n">dw</span>  <span class="c1"># 更新二阶动量（类似于RMSprop）</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">moment1</span> <span class="o">/</span> <span class="p">(</span><span class="n">moment2</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>  <span class="c1"># 使用自适应学习率更新参数</span>
</code></pre></div>
<ul>
<li>
<p><strong>一阶动量 (moment1)：</strong>
  <code>moment1</code> 是梯度的指数加权移动平均，类似于动量优化中的累积梯度，旨在平滑梯度更新。<code>beta1</code> 控制着一阶动量的衰减率，通常取值为 0.9。</p>
</li>
<li>
<p><strong>二阶动量 (moment2)：</strong>
  <code>moment2</code> 是梯度平方的指数加权移动平均，用于调整学习率。<code>beta2</code> 控制着二阶动量的衰减率，通常取值为 0.999。这可以看作是 RMSprop 中的均方根移动平均。</p>
</li>
</ul>
<p>在 Adam 中，一般还需要对一阶和二阶动量进行<strong><code>偏差修正</code></strong>，特别是在前几次迭代时。偏差修正可以提高算法的收敛性能。</p>
<div class="highlight"><pre><span></span><code><span class="n">moment1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">moment2</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">moment1</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">moment1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span>
    <span class="n">moment2</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">moment2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">*</span> <span class="n">dw</span>

    <span class="c1"># 偏差修正</span>
    <span class="n">m1_hat</span> <span class="o">=</span> <span class="n">moment1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
    <span class="n">m2_hat</span> <span class="o">=</span> <span class="n">moment2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m1_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">m2_hat</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div>
<p>这种偏差修正的目的是减少初始阶段估计值的偏差，尤其是在迭代次数较少的情况下。</p>
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191129969.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image-20240819112958840" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408191129969.png" /></a></p>
<h2 id="44-小结">4.4 小结<a class="headerlink" href="#44-小结" title="Permanent link">&para;</a></h2>
<hr />
<p><a class="glightbox" href="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171610991.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="img" src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202408171610991.png" /></a></p>
<p>信息流的总结图例。数据集中的(x, y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量 <strong>f</strong> 中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分 f 和实际标签 y 之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<hr />
<p>在本节课中：</p>
<ul>
<li>将损失函数比作了一个 <strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见 SVM 的损失函数是分段线性的，并且是碗状的。</li>
</ul>
<p>通过迭代优化，从随机初始权重出发，逐步降低损失值。</p>
<ul>
<li>
<p>梯度表示函数在当前点最陡峭的上升方向，通过反向操作来更新权重，使得损失最小化。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<em>h</em>，用来计算数值梯度）。</p>
</li>
<li>
<p>参数更新需要有技巧地设置 <strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</p>
</li>
<li>
<p>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用 <strong>梯度检查</strong> 来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</p>
</li>
<li>
<p>介绍了 <strong>梯度下降</strong> 算法，它在循环中迭代地计算梯度并更新参数。</p>
</li>
</ul>
<p><strong>预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的 <strong>反向传播（</strong> <strong>backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy 2024 rescuerz
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.expand", "navigation.top", "search.highlight", "search.share", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../../supports/js/xlink.js"></script>
      
        <script src="../../supports/js/katex.js"></script>
      
        <script src="https://jsd.cdn.zzko.cn/npm/katex@0.16.4/dist/katex.min.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>